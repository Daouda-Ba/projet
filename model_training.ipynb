{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modélisation du prix des maisons\n",
        "\n",
        "Ce notebook documente l'ensemble du flux de travail : exploration des données, préparation, entraînement de modèles réutilisable pour l'application Streamlit. Les cellules Markdown détaillent chaque étape afin de servir de rapport méthodologique complet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Importation des librairies et configuration\n",
        "Nous commençons par charger les packages nécessaires pour l'analyse exploratoire et la modélisation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "import joblib\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Chargement des données\n",
        "Nous utilisons les fichiers `train.csv` et `test.csv` fournis. La variable cible est `SalePrice` dans l'échantillon d'entraînement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_DIR = pathlib.Path('.')\n",
        "train_path = DATA_DIR / 'train.csv'\n",
        "test_path = DATA_DIR / 'test.csv'\n",
        "\n",
        "train_df = pd.read_csv(train_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "\n",
        "train_df.shape, test_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Aperçu des données\n",
        "Un premier coup d'œil sur les colonnes principales permet d'identifier rapidement la structure du jeu de données."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df.describe(include='all').transpose().iloc[:15]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Données manquantes\n",
        "Nous calculons le pourcentage de valeurs manquantes par colonne et visualisons les variables les plus touchées."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "missing_ratio = (train_df.isnull().mean() * 100).sort_values(ascending=False)\n",
        "missing_ratio.head(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "top_missing = missing_ratio.head(15).sort_values()\n",
        "ax = top_missing.plot(kind='barh', color='steelblue')\n",
        "ax.set_title('Top 15 des colonnes avec valeurs manquantes (%)')\n",
        "ax.set_xlabel('Pourcentage de valeurs manquantes')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Analyse de la variable cible\n",
        "`SalePrice` présente une distribution asymétrique ; nous observons également sa version log-transformée pour stabiliser la variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "sns.histplot(train_df['SalePrice'], kde=True, ax=axes[0], color='forestgreen')\n",
        "axes[0].set_title('Distribution de SalePrice')\n",
        "sns.histplot(np.log1p(train_df['SalePrice']), kde=True, ax=axes[1], color='darkorange')\n",
        "axes[1].set_title('Distribution de log(SalePrice + 1)')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Colonnes numériques et catégorielles\n",
        "Nous séparons les variables numériques et catégorielles afin de définir des pipelines de prétraitement adaptés."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "categorical_cols = train_df.select_dtypes(include=['object']).columns.tolist()\n",
        "numeric_cols = [col for col in train_df.columns if col not in categorical_cols + ['SalePrice']]\n",
        "\n",
        "print(f\"Variables numériques ({len(numeric_cols)}): {numeric_cols[:10]}{'...' if len(numeric_cols) > 10 else ''}\")\n",
        "print(f\"Variables catégorielles ({len(categorical_cols)}): {categorical_cols[:10]}{'...' if len(categorical_cols) > 10 else ''}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Corrélations avec la cible\n",
        "Nous observons les variables numériques les plus corrélées avec `SalePrice` pour guider l'interprétation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corr_matrix = train_df[numeric_cols + ['SalePrice']].corr()\n",
        "top_corr = corr_matrix['SalePrice'].abs().sort_values(ascending=False).head(12).index\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr_matrix.loc[top_corr, top_corr], annot=True, fmt='.2f', cmap='crest')\n",
        "plt.title('Corrélations des variables numériques avec SalePrice')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Prétraitement\n",
        "- **Numérique :** imputation par la médiane.\n",
        "- **Catégoriel :** imputation par la modalité la plus fréquente, puis encodage *one-hot* avec gestion des modalités inconnues.\n",
        "\n",
        "La séparation entre `X` et `y` est réalisée avant la division en apprentissage/test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = train_df.drop(columns=['SalePrice'])\n",
        "y = train_df['SalePrice']\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median'))\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('numeric', numeric_transformer, numeric_cols),\n",
        "        ('categorical', categorical_transformer, categorical_cols),\n",
        "    ]\n",
        ")\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "X_train.shape, X_valid.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Fonction d'évaluation\n",
        "Pour comparer les modèles, nous utilisons la RMSE (erreur quadratique moyenne racine), la MAE et le coefficient $R^2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, X_tr, y_tr, X_te, y_te):\n",
        "    pred_train = model.predict(X_tr)\n",
        "    pred_test = model.predict(X_te)\n",
        "\n",
        "    metrics = {\n",
        "        'RMSE_train': mean_squared_error(y_tr, pred_train, squared=False),\n",
        "        'MAE_train': mean_absolute_error(y_tr, pred_train),\n",
        "        'R2_train': r2_score(y_tr, pred_train),\n",
        "        'RMSE_test': mean_squared_error(y_te, pred_test, squared=False),\n",
        "        'MAE_test': mean_absolute_error(y_te, pred_test),\n",
        "        'R2_test': r2_score(y_te, pred_test),\n",
        "    }\n",
        "    return pd.Series(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Modèle 1 : Random Forest Regressor\n",
        "Forêt aléatoire robuste aux relations non linéaires et interactions. Nous évaluons ses performances sur l'échantillon de validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rf_model = Pipeline(steps=[\n",
        "    ('preprocess', preprocess),\n",
        "    ('model', RandomForestRegressor(\n",
        "        n_estimators=400,\n",
        "        max_depth=None,\n",
        "        min_samples_leaf=1,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_scores = evaluate(rf_model, X_train, y_train, X_valid, y_valid)\n",
        "rf_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Modèle 2 : XGBoost Regressor\n",
        "Gradient boosting performant sur données tabulaires. Les hyperparamètres sont calibrés pour un bon compromis biais/variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xgb_model = Pipeline(steps=[\n",
        "    ('preprocess', preprocess),\n",
        "    ('model', XGBRegressor(\n",
        "        n_estimators=500,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=4,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        objective='reg:squarederror'\n",
        "    ))\n",
        "])\n",
        "\n",
        "xgb_model.fit(X_train, y_train)\n",
        "xgb_scores = evaluate(xgb_model, X_train, y_train, X_valid, y_valid)\n",
        "xgb_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Comparaison des modèles\n",
        "Nous rassemblons les métriques clés pour sélectionner le modèle final."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.DataFrame({'RandomForest': rf_scores, 'XGBoost': xgb_scores})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Importance des variables du meilleur modèle\n",
        "Nous extrayons les importances issues du modèle XGBoost (meilleur score attendu) en tenant compte des variables encodées."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_feature_importance(model, numeric, categorical, top_n=20):\n",
        "    preprocess_step = model.named_steps['preprocess']\n",
        "    ohe = preprocess_step.named_transformers_['categorical'].named_steps['encoder']\n",
        "    feature_names = numeric + list(ohe.get_feature_names_out(categorical))\n",
        "    booster = model.named_steps['model']\n",
        "    importances = booster.feature_importances_\n",
        "    importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
        "    return importance_df.sort_values(by='importance', ascending=False).head(top_n)\n",
        "\n",
        "importance_df = get_feature_importance(xgb_model, numeric_cols, categorical_cols, top_n=25)\n",
        "ax = importance_df.sort_values('importance').plot.barh(x='feature', y='importance', color='indianred')\n",
        "ax.set_title('Top 25 - Importance des variables (XGBoost)')\n",
        "plt.tight_layout()\n",
        "importance_df"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
