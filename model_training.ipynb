{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mod\u00e9lisation du prix des maisons\n",
        "\n",
        "Ce notebook documente l'ensemble du flux de travail : exploration des donn\u00e9es, pr\u00e9paration, entra\u00eenement de mod\u00e8les et g\u00e9n\u00e9ration d'une soumission r\u00e9utilisable pour l'application Streamlit. Les cellules Markdown d\u00e9taillent chaque \u00e9tape afin de servir de rapport m\u00e9thodologique complet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Importation des librairies et configuration\n",
        "Nous commen\u00e7ons par charger les packages n\u00e9cessaires pour l'analyse exploratoire et la mod\u00e9lisation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "import joblib\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Chargement des donn\u00e9es\n",
        "Nous utilisons les fichiers `train.csv` et `test.csv` fournis. La variable cible est `SalePrice` dans l'\u00e9chantillon d'entra\u00eenement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "DATA_DIR = pathlib.Path('.')\n",
        "train_path = DATA_DIR / 'train.csv'\n",
        "test_path = DATA_DIR / 'test.csv'\n",
        "\n",
        "train_df = pd.read_csv(train_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "\n",
        "train_df.shape, test_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Aper\u00e7u des donn\u00e9es\n",
        "Un premier coup d'\u0153il sur les colonnes principales permet d'identifier rapidement la structure du jeu de donn\u00e9es."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "train_df.describe(include='all').transpose().iloc[:15]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Donn\u00e9es manquantes\n",
        "Nous calculons le pourcentage de valeurs manquantes par colonne et visualisons les variables les plus touch\u00e9es."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "missing_ratio = (train_df.isnull().mean() * 100).sort_values(ascending=False)\n",
        "missing_ratio.head(15)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "top_missing = missing_ratio.head(15).sort_values()\n",
        "ax = top_missing.plot(kind='barh', color='steelblue')\n",
        "ax.set_title('Top 15 des colonnes avec valeurs manquantes (%)')\n",
        "ax.set_xlabel('Pourcentage de valeurs manquantes')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Analyse de la variable cible\n",
        "`SalePrice` pr\u00e9sente une distribution asym\u00e9trique ; nous observons \u00e9galement sa version log-transform\u00e9e pour stabiliser la variance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "sns.histplot(train_df['SalePrice'], kde=True, ax=axes[0], color='forestgreen')\n",
        "axes[0].set_title('Distribution de SalePrice')\n",
        "sns.histplot(np.log1p(train_df['SalePrice']), kde=True, ax=axes[1], color='darkorange')\n",
        "axes[1].set_title('Distribution de log(SalePrice + 1)')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Colonnes num\u00e9riques et cat\u00e9gorielles\n",
        "Nous s\u00e9parons les variables num\u00e9riques et cat\u00e9gorielles afin de d\u00e9finir des pipelines de pr\u00e9traitement adapt\u00e9s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "categorical_cols = train_df.select_dtypes(include=['object']).columns.tolist()\n",
        "numeric_cols = [col for col in train_df.columns if col not in categorical_cols + ['SalePrice']]\n",
        "\n",
        "print(f\"Variables num\u00e9riques ({len(numeric_cols)}): {numeric_cols[:10]}{'...' if len(numeric_cols) > 10 else ''}\")\n",
        "print(f\"Variables cat\u00e9gorielles ({len(categorical_cols)}): {categorical_cols[:10]}{'...' if len(categorical_cols) > 10 else ''}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Corr\u00e9lations avec la cible\n",
        "Nous observons les variables num\u00e9riques les plus corr\u00e9l\u00e9es avec `SalePrice` pour guider l'interpr\u00e9tation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "corr_matrix = train_df[numeric_cols + ['SalePrice']].corr()\n",
        "top_corr = corr_matrix['SalePrice'].abs().sort_values(ascending=False).head(12).index\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr_matrix.loc[top_corr, top_corr], annot=True, fmt='.2f', cmap='crest')\n",
        "plt.title('Corr\u00e9lations des variables num\u00e9riques avec SalePrice')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Pr\u00e9traitement\n",
        "- **Num\u00e9rique :** imputation par la m\u00e9diane.\n",
        "- **Cat\u00e9goriel :** imputation par la modalit\u00e9 la plus fr\u00e9quente, puis encodage *one-hot* avec gestion des modalit\u00e9s inconnues.\n",
        "\n",
        "La s\u00e9paration entre `X` et `y` est r\u00e9alis\u00e9e avant la division en apprentissage/test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "X = train_df.drop(columns=['SalePrice'])\n",
        "y = train_df['SalePrice']\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median'))\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('numeric', numeric_transformer, numeric_cols),\n",
        "        ('categorical', categorical_transformer, categorical_cols),\n",
        "    ]\n",
        ")\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "X_train.shape, X_valid.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Fonction d'\u00e9valuation\n",
        "Pour comparer les mod\u00e8les, nous utilisons la RMSE (erreur quadratique moyenne racine), la MAE et le coefficient $R^2$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def evaluate(model, X_tr, y_tr, X_te, y_te):\n",
        "    pred_train = model.predict(X_tr)\n",
        "    pred_test = model.predict(X_te)\n",
        "\n",
        "    metrics = {\n",
        "        'RMSE_train': mean_squared_error(y_tr, pred_train, squared=False),\n",
        "        'MAE_train': mean_absolute_error(y_tr, pred_train),\n",
        "        'R2_train': r2_score(y_tr, pred_train),\n",
        "        'RMSE_test': mean_squared_error(y_te, pred_test, squared=False),\n",
        "        'MAE_test': mean_absolute_error(y_te, pred_test),\n",
        "        'R2_test': r2_score(y_te, pred_test),\n",
        "    }\n",
        "    return pd.Series(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Mod\u00e8le 1 : Random Forest Regressor\n",
        "For\u00eat al\u00e9atoire robuste aux relations non lin\u00e9aires et interactions. Nous \u00e9valuons ses performances sur l'\u00e9chantillon de validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "rf_model = Pipeline(steps=[\n",
        "    ('preprocess', preprocess),\n",
        "    ('model', RandomForestRegressor(\n",
        "        n_estimators=400,\n",
        "        max_depth=None,\n",
        "        min_samples_leaf=1,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_scores = evaluate(rf_model, X_train, y_train, X_valid, y_valid)\n",
        "rf_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Mod\u00e8le 2 : XGBoost Regressor\n",
        "Gradient boosting performant sur donn\u00e9es tabulaires. Les hyperparam\u00e8tres sont calibr\u00e9s pour un bon compromis biais/variance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "xgb_model = Pipeline(steps=[\n",
        "    ('preprocess', preprocess),\n",
        "    ('model', XGBRegressor(\n",
        "        n_estimators=500,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=4,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        objective='reg:squarederror'\n",
        "    ))\n",
        "])\n",
        "\n",
        "xgb_model.fit(X_train, y_train)\n",
        "xgb_scores = evaluate(xgb_model, X_train, y_train, X_valid, y_valid)\n",
        "xgb_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Comparaison des mod\u00e8les\n",
        "Nous rassemblons les m\u00e9triques cl\u00e9s pour s\u00e9lectionner le mod\u00e8le final."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "pd.DataFrame({'RandomForest': rf_scores, 'XGBoost': xgb_scores})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Importance des variables du meilleur mod\u00e8le\n",
        "Nous extrayons les importances issues du mod\u00e8le XGBoost (meilleur score attendu) en tenant compte des variables encod\u00e9es."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def get_feature_importance(model, numeric, categorical, top_n=20):\n",
        "    preprocess_step = model.named_steps['preprocess']\n",
        "    ohe = preprocess_step.named_transformers_['categorical'].named_steps['encoder']\n",
        "    feature_names = numeric + list(ohe.get_feature_names_out(categorical))\n",
        "    booster = model.named_steps['model']\n",
        "    importances = booster.feature_importances_\n",
        "    importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
        "    return importance_df.sort_values(by='importance', ascending=False).head(top_n)\n",
        "\n",
        "importance_df = get_feature_importance(xgb_model, numeric_cols, categorical_cols, top_n=25)\n",
        "ax = importance_df.sort_values('importance').plot.barh(x='feature', y='importance', color='indianred')\n",
        "ax.set_title('Top 25 - Importance des variables (XGBoost)')\n",
        "plt.tight_layout()\n",
        "importance_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Entra\u00eenement sur l'ensemble des donn\u00e9es et export des artefacts\n",
        "Nous r\u00e9-entra\u00eenons le mod\u00e8le XGBoost sur tout l'\u00e9chantillon d'entra\u00eenement, g\u00e9n\u00e9rons une pr\u00e9diction pour `test.csv` et sauvegardons :\n",
        "- le mod\u00e8le complet (`mon_deuxieme_model.joblib`)\n",
        "- l'encodeur One-Hot (`one_hot_encoder.joblib`)\n",
        "- la liste des features encod\u00e9es (`features_list.pkl`)\n",
        "- le fichier de soumission (`submission.csv`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Entra\u00eenement final\n",
        "xgb_final = Pipeline(steps=[\n",
        "    ('preprocess', preprocess),\n",
        "    ('model', XGBRegressor(\n",
        "        n_estimators=500,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=4,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        objective='reg:squarederror'\n",
        "    ))\n",
        "])\n",
        "\n",
        "xgb_final.fit(X, y)\n",
        "\n",
        "# Pr\u00e9paration du test et pr\u00e9diction\n",
        "test_predictions = xgb_final.predict(test_df)\n",
        "submission = pd.DataFrame({\n",
        "    'Id': test_df['Id'],\n",
        "    'SalePrice': test_predictions\n",
        "})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "# Sauvegarde des artefacts pour l'application Streamlit\n",
        "joblib.dump(xgb_final, 'mon_deuxieme_model.joblib')\n",
        "encoder = xgb_final.named_steps['preprocess'].named_transformers_['categorical'].named_steps['encoder']\n",
        "joblib.dump(encoder, 'one_hot_encoder.joblib')\n",
        "\n",
        "# Liste des features apr\u00e8s encodage\n",
        "encoded_feature_names = numeric_cols + list(encoder.get_feature_names_out(categorical_cols))\n",
        "import pickle\n",
        "with open('features_list.pkl', 'wb') as f:\n",
        "    pickle.dump(encoded_feature_names, f)\n",
        "\n",
        "submission.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Points cl\u00e9s et prochaines \u00e9tapes\n",
        "- La pipeline de pr\u00e9traitement g\u00e8re automatiquement les valeurs manquantes et l'encodage cat\u00e9goriel.\n",
        "- XGBoost offre de meilleures performances et est retenu comme mod\u00e8le principal.\n",
        "- Les artefacts sauvegard\u00e9s sont pr\u00eats pour une utilisation dans l'interface Streamlit.\n",
        "\n",
        "Pour aller plus loin, on pourrait explorer :\n",
        "- L'optimisation d'hyperparam\u00e8tres (par validation crois\u00e9e).\n",
        "- L'ajout de variables d\u00e9riv\u00e9es (qualit\u00e9 moyenne des pi\u00e8ces, \u00e2ge du b\u00e2timent, etc.).\n",
        "- L'analyse d'erreurs cibl\u00e9e pour identifier les propri\u00e9t\u00e9s mal estim\u00e9es."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}